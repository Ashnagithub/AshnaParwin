import org.apache.log4j.Level
import org.apache.log4j.Logger
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

object humidity_check extends App {  



	 Logger.getLogger("org").setLevel(Level.ERROR)   
	
	 val sparkConf=new SparkConf()
	 sparkConf.set("spark.app.name","Assignment1_Week12")
	 sparkConf.set("spark.master","local[2]")
	 val spark=SparkSession.builder().config(sparkConf).getOrCreate()
	 
	 val df=spark.read
	       .format("csv")
	       .option("header",true)
	       .option("inferschema",true)
	       .option("path","D:/Ashna/group*.csv")
	       .load()
	       
	       
	 df.createOrReplaceTempView("table1")
	 val result=spark.sql("select sensor_id,min(humidity_value) as min,avg(humidity_value) as avg,max(humidity_value) as max from table1 where humidity_value!='NaN'  group by sensor_id")
	 
    
 
   val result1=spark.sql("select sensor_id,min(humidity_value) as min,avg(humidity_value) as avg,max(humidity_value) as max from table1 where humidity_value='NaN' group by sensor_id")

   val dfunion=result.union(result1)
 
   dfunion.createOrReplaceTempView("table3")
   
   val result3=spark.sql("select sensor_id,min,avg,max from(select *,row_number() over (partition by sensor_id order by sensor_id) as RowNbr from table3) source where RowNbr = 1")
   
   result3.createOrReplaceTempView("tablefinal")
 
   val finaltable=spark.sql("select * from tablefinal order by avg")
   
   finaltable.show()


	 
}

